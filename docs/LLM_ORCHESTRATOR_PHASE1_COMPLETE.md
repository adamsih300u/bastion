# LLM Orchestrator Microservice - Phase 1 Complete

## Overview

**Phase 1 Status: âœ… COMPLETE - Safe Parallel Deployment**

The LLM Orchestrator service has been created and runs **in parallel** with the existing backend. **Zero impact on production** - all current functionality remains unchanged.

## What Was Created

### 1. Service Structure

```
llm-orchestrator/
â”œâ”€â”€ agents/                  # Future: All LangGraph agents
â”œâ”€â”€ orchestrator/            # Core orchestration logic
â”‚   â””â”€â”€ grpc_service.py     # gRPC service implementation (skeleton)
â”œâ”€â”€ state/                   # Future: State management
â”œâ”€â”€ tools/                   # Future: Tool implementations  
â”œâ”€â”€ models/                  # Future: Pydantic models
â”œâ”€â”€ grpc/                    # gRPC protocol buffers
â”‚   â”œâ”€â”€ orchestrator.proto  # Main orchestrator service definition
â”‚   â””â”€â”€ tool_service.proto  # Backend tool callbacks definition
â”œâ”€â”€ config/
â”‚   â””â”€â”€ settings.py         # Service configuration
â”œâ”€â”€ Dockerfile              # Container definition
â”œâ”€â”€ requirements.txt        # Python dependencies
â”œâ”€â”€ main.py                 # Service entry point
â”œâ”€â”€ test_grpc.py           # gRPC connectivity tests
â””â”€â”€ README.md              # Service documentation
```

### 2. gRPC Interfaces

**Orchestrator Service (`orchestrator.proto`):**
- `StreamChat` - Streaming LLM responses (Phase 1: echo only)
- `StartTask` - Async task processing (stub)
- `GetTaskStatus` - Task status queries (stub)
- `ApprovePermission` - HITL permission handling (stub)
- `GetPendingPermissions` - List pending permissions (stub)
- `HealthCheck` - Service health status âœ…

**Tool Service (`tool_service.proto`):**
- Document search and retrieval
- RSS feed operations
- Entity search
- Weather data
- Org-mode operations

*Note: Tool service implementation in Phase 2*

### 3. Docker Integration

**Added to `docker-compose.yml`:**
- New `llm-orchestrator` service
- Runs on port 50051 (gRPC)
- Depends on PostgreSQL and backend
- Health checks configured
- Independent restart capability

**Key Feature:** Existing services **NOT modified** - orchestrator runs alongside

### 4. Feature Flag System

**Created `backend/config/feature_flags.py`:**
- `USE_GRPC_ORCHESTRATOR` - Enable routing to new service (OFF)
- `GRPC_ORCHESTRATOR_PERCENTAGE` - Gradual rollout 0-100% (0%)
- `USE_GRPC_TOOL_SERVICE` - Enable tool callbacks (OFF)
- `GRPC_FALLBACK_TO_LOCAL` - Auto-fallback on errors (ON)

**Phase 1:** All flags OFF - safe parallel testing only

## Current State

### What Works âœ…

1. **Service Startup**: Container builds and starts successfully
2. **Health Checks**: gRPC health endpoint responds
3. **Basic Streaming**: Echo test responses via StreamChat
4. **Database Connection**: Can connect to PostgreSQL (checkpointer ready)
5. **Independent Lifecycle**: Restart without affecting main app

### What's NOT Implemented Yet â³

1. **Agent Logic**: No LangGraph agents migrated yet
2. **Tool Callbacks**: Can't access backend data yet
3. **State Management**: Checkpointer not wired up yet
4. **Production Routing**: No traffic from main API yet
5. **Full Orchestration**: No intent classification or routing yet

## Testing Phase 1

### Verify Service is Running

```bash
# Check container status
docker compose ps llm-orchestrator

# Should show: Up (healthy)
```

### Test gRPC Connectivity

```bash
# Option 1: Use Python test script
docker compose exec llm-orchestrator python test_grpc.py

# Option 2: Use grpcurl (if installed)
grpcurl -plaintext localhost:50051 orchestrator.OrchestratorService/HealthCheck
```

### Test Independent Restart

```bash
# Restart ONLY orchestrator (not backend/frontend)
docker compose restart llm-orchestrator

# Verify main app still works:
# - Open browser to http://localhost:3000
# - Documents page should load normally
# - Chat should work normally (uses existing backend)
```

## Safety Guarantees

### What CAN'T Break Production

âœ… **Existing backend unchanged** - No code modifications
âœ… **API endpoints unchanged** - Same REST interface
âœ… **Database unchanged** - No schema modifications
âœ… **Frontend unchanged** - No client-side changes
âœ… **User experience unchanged** - Zero impact on workflows

### Rollback Strategy

If any issues arise:

```bash
# Option 1: Stop orchestrator service
docker compose stop llm-orchestrator

# Option 2: Remove from docker-compose.yml
# Comment out lines 265-316 in docker-compose.yml

# Option 3: Rebuild without orchestrator
docker compose up --build backend frontend db redis
```

## Next Steps

### Phase 2: Tool Integration (Week 2)

**Goal:** Enable orchestrator to access backend data

**Tasks:**
1. Implement tool service gRPC endpoints in backend
2. Create gRPC client wrappers in orchestrator
3. Test document search, RSS, entity queries
4. Enable `USE_GRPC_TOOL_SERVICE` flag

**Success Criteria:**
- Orchestrator can search documents via gRPC
- Orchestrator can query RSS feeds
- Tool calls measured < 50ms overhead

### Phase 3: Agent Migration (Week 3-4)

**Goal:** Move all LangGraph agents to orchestrator

**Tasks:**
1. Copy 25 agents from `backend/services/langgraph_agents/`
2. Wire up tool registry with gRPC tools
3. Implement state management with checkpointer
4. Test full agent workflows in isolation
5. Implement streaming responses

**Success Criteria:**
- All agents functional in orchestrator
- State persists across conversations
- Streaming responses work end-to-end

### Phase 4: Production Cutover (Week 5)

**Goal:** Route production traffic to new service

**Tasks:**
1. Update backend API to proxy to gRPC orchestrator
2. Enable `USE_GRPC_ORCHESTRATOR` = true
3. Set `GRPC_ORCHESTRATOR_PERCENTAGE` = 10%
4. Monitor latency, errors, LLM costs
5. Gradually increase: 10% â†’ 25% â†’ 50% â†’ 100%

**Success Criteria:**
- < 5% error rate
- Latency < 200ms overhead
- Successful HITL flows
- Users don't notice migration

### Phase 5: Cleanup (Week 6)

**Goal:** Remove legacy orchestrator code

**Tasks:**
1. Remove old orchestrator from backend
2. Remove feature flags (direct routing)
3. Update documentation
4. Archive migration notes

## Monitoring Recommendations

### During Phase 1 (Now)

```bash
# Watch orchestrator logs
docker compose logs -f llm-orchestrator

# Check resource usage
docker stats bastion-llm-orchestrator

# Verify health
docker compose ps llm-orchestrator
```

### During Phase 2-4 (Future)

- Add Prometheus metrics to gRPC service
- Track request latency (p50, p95, p99)
- Monitor LLM token usage per agent
- Alert on error rates > 5%
- Dashboard for gradual rollout percentage

## Benefits Realized

### Development Velocity

**Before:**
```bash
# Tune research agent prompt
vim backend/services/langgraph_agents/clean_research_agent.py
docker compose down
docker compose up --build
# Wait 2-3 minutes for full rebuild
```

**After (Phase 3+):**
```bash
# Tune research agent prompt
vim llm-orchestrator/agents/clean_research_agent.py
docker compose restart llm-orchestrator
# Wait 15 seconds, backend/frontend still running
```

### Independent Scaling

Once migration complete:
- Scale orchestrator: `docker compose up --scale llm-orchestrator=3`
- Scale backend separately for document operations
- Resource allocation per service

### Technology Flexibility

Future possibilities:
- Rewrite hot-path agents in Go for performance
- Swap LLM providers without touching main app
- A/B test different agent configurations
- GPU-optimized container for local models

## Architecture Diagram

```
Current (Phase 1):
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Frontend â”‚
â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜
      â”‚
      â”‚ REST (all traffic)
      â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Backend          â”‚
â”‚ (Production)     â”‚â—„â”€â”€ All requests here
â”‚ - LangGraph      â”‚
â”‚ - Agents (25)    â”‚
â”‚ - Documents      â”‚
â”‚ - RSS            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ LLM Orchestrator â”‚â—„â”€â”€ Parallel testing only
â”‚ (Testing)        â”‚    No production traffic
â”‚ - gRPC skeleton  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜


Target (Phase 4+):
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Frontend â”‚
â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜
      â”‚
      â”‚ REST
      â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Backend (API)    â”‚
â”‚ - Documents      â”‚
â”‚ - RSS            â”‚
â”‚ - Auth           â”‚
â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
     â”‚
     â”‚ gRPC
     â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ LLM Orchestrator â”‚â—„â”€â”€ All LLM traffic here
â”‚ (Production)     â”‚
â”‚ - Agents (25)    â”‚
â”‚ - LangGraph      â”‚
â”‚ - Tools          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## Questions & Answers

**Q: Is production affected?**
A: No. The new service runs in parallel with zero production traffic.

**Q: Can I disable the new service?**
A: Yes. `docker compose stop llm-orchestrator` or comment it out.

**Q: When will traffic move to new service?**
A: Phase 4 (Week 5) with gradual 10%â†’100% rollout.

**Q: What if the new service fails?**
A: Feature flags ensure automatic fallback to existing backend.

**Q: Can I test the new service now?**
A: Yes! Use `test_grpc.py` or `grpcurl` for basic connectivity tests.

**Q: How long until migration complete?**
A: ~6 weeks for full migration, but production unaffected throughout.

## Conclusion

**Phase 1 successfully establishes:**

âœ… **Parallel architecture** - New service alongside existing
âœ… **Zero risk deployment** - No production changes
âœ… **Testing infrastructure** - gRPC service skeleton
âœ… **Gradual rollout plan** - Feature flags ready
âœ… **Independent lifecycle** - Restart without disruption
âœ… **Clear migration path** - Phases 2-5 documented

**Next:** Proceed to Phase 2 (Tool Integration) when ready.

---

**BULLY!** The foundation is laid - onward to Phase 2! ğŸ¯

