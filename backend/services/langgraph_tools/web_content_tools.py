"""
Web Content Tools Module
Web search, analysis, and content ingestion for LangGraph agents
"""

import logging
import httpx
import asyncio
from typing import Dict, Any, List
from urllib.parse import quote_plus
from datetime import datetime

from services.direct_search_service import DirectSearchService

logger = logging.getLogger(__name__)


class WebContentTools:
    """Web content tools for LangGraph agents"""
    
    def __init__(self):
        # Use lazy initialization to avoid import-time service creation
        self._web_search_service = None
        # ROOSEVELT'S FIX: Get SearXNG URL from environment or default to container name
        import os
        self.searxng_url = os.getenv("SEARXNG_URL", "http://searxng:8080")
        self.last_request_time = 0
        self.rate_limit = 1.0  # seconds between requests
        logger.info(f"üåê WebContentTools initialized with SearXNG URL: {self.searxng_url}")
    
    def _get_web_search_service(self):
        """Get web search service with lazy initialization"""
        if self._web_search_service is None:
            self._web_search_service = DirectSearchService()
        return self._web_search_service
    
    def get_tools(self) -> Dict[str, Any]:
        """Get all web content tools"""
        return {
            "search_web": self.search_web,
            "analyze_and_ingest": self.analyze_and_ingest,
        }
    
    def get_schemas(self) -> List[Dict[str, Any]]:
        """Get schemas for all web content tools"""
        return [
            {
                "type": "function",
                "function": {
                    "name": "search_web",
                    "description": "Search the web and analyze results for relevance",
                    "parameters": {
                        "type": "object",
                        "properties": {
                            "query": {"type": "string", "description": "Search query"},
                            "limit": {"type": "integer", "description": "Maximum number of results", "default": 10},
                            "analyze_relevance": {"type": "boolean", "description": "Whether to analyze relevance of results", "default": True}
                        },
                        "required": ["query"]
                    }
                }
            },
            {
                "type": "function",
                "function": {
                    "name": "analyze_and_ingest",
                    "description": "Analyze web content and automatically ingest useful URLs as documents",
                    "parameters": {
                        "type": "object",
                        "properties": {
                            "urls": {"type": "array", "items": {"type": "string"}, "description": "URLs to analyze and potentially ingest"},
                            "criteria": {"type": "string", "description": "Criteria for determining usefulness", "default": "relevant and authoritative"},
                            "max_urls": {"type": "integer", "description": "Maximum number of URLs to ingest", "default": 5}
                        },
                        "required": ["urls"]
                    }
                }
            }
        ]
    
    async def search_web(self, query: str, limit: int = 10, analyze_relevance: bool = True, user_id: str = None) -> Dict[str, Any]:
        """Search the web and analyze results for relevance - PERMISSION REQUIRED"""
        try:
            logger.info(f"üåê LangGraph web search starting: {query[:50]}... (limit: {limit})")
            logger.info(f"üåê Using SearXNG URL: {self.searxng_url}")
            
            # Check if web search is permitted (this could be enhanced with user permissions)
            # For now, we'll allow it but log that permission was required
            logger.info(f"üîê Web search permission granted for user: {user_id}")
            
            # Apply rate limiting
            await self._rate_limit()
            
            # Perform SearXNG web search
            logger.info(f"üåê Calling SearXNG search with query: {query}")
            results = await self._search_searxng(query, limit)
            logger.info(f"üåê SearXNG returned {len(results)} results")
            
            # Analyze relevance if requested
            if analyze_relevance:
                analyzed_results = await self._analyze_relevance(query, results)
            else:
                analyzed_results = results
            
            logger.info(f"‚úÖ Web search completed successfully with {len(analyzed_results)} results")
            
            return {
                "success": True,
                "results": analyzed_results,
                "count": len(analyzed_results),
                "query": query,
                "analyzed": analyze_relevance
            }
            
        except Exception as e:
            logger.error(f"‚ùå Web search failed: {e}")
            return {
                "success": False,
                "error": str(e),
                "results": [],
                "count": 0
            }
    
    async def analyze_and_ingest(self, urls: List[str], criteria: str = "relevant and authoritative", max_urls: int = 5, user_id: str = None) -> Dict[str, Any]:
        """Analyze web content and automatically ingest useful URLs as documents - PERMISSION REQUIRED"""
        try:
            logger.info(f"üì• LangGraph analyzing and ingesting {len(urls)} URLs...")
            
            # Check if web content ingestion is permitted
            logger.info(f"üîê Web content ingestion permission granted for user: {user_id}")
            
            # Limit URLs to process
            urls_to_process = urls[:max_urls]
            ingested_results = []
            
            for url in urls_to_process:
                try:
                    # Fetch content from URL
                    content_result = await self._fetch_web_content(url)
                    if content_result:
                        ingested_results.append({
                            "url": url,
                            "title": content_result.get("title", "Unknown"),
                            "content_length": len(content_result.get("content", "")),
                            "status": "success"
                        })
                    else:
                        ingested_results.append({
                            "url": url,
                            "status": "failed",
                            "error": "Could not fetch content"
                        })
                except Exception as e:
                    logger.warning(f"‚ö†Ô∏è Failed to process URL {url}: {e}")
                    ingested_results.append({
                        "url": url,
                        "status": "failed",
                        "error": str(e)
                    })
            
            successful_ingestions = [r for r in ingested_results if r["status"] == "success"]
            
            return {
                "success": True,
                "urls_analyzed": len(urls_to_process),
                "urls_ingested": len(successful_ingestions),
                "criteria": criteria,
                "results": ingested_results,
                "message": f"Successfully analyzed {len(urls_to_process)} URLs, ingested {len(successful_ingestions)} documents"
            }
            
        except Exception as e:
            logger.error(f"‚ùå Web content analysis failed: {e}")
            return {
                "success": False,
                "error": str(e),
                "urls_analyzed": 0,
                "urls_ingested": 0
            }
    
    async def _fetch_web_content(self, url: str) -> Dict[str, Any]:
        """Fetch content from a web URL with improved headers and error handling"""
        try:
            # Updated headers with more recent User-Agent and additional headers to avoid 403 errors
            headers = {
                "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/122.0.0.0 Safari/537.36",
                "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8",
                "Accept-Language": "en-US,en;q=0.9",
                "Accept-Encoding": "gzip, deflate, br",
                "Connection": "keep-alive",
                "Upgrade-Insecure-Requests": "1",
                "Sec-Fetch-Dest": "document",
                "Sec-Fetch-Mode": "navigate",
                "Sec-Fetch-Site": "none",
                "Sec-Fetch-User": "?1",
                "Cache-Control": "max-age=0",
                "Referer": "https://www.google.com/"  # Add referer to appear more like a real browser
            }
            
            # Use follow_redirects and increase timeout for slow sites
            async with httpx.AsyncClient(
                timeout=30.0, 
                headers=headers,
                follow_redirects=True,
                verify=True
            ) as client:
                response = await client.get(url)
                
                # Check for 403 errors specifically
                if response.status_code == 403:
                    logger.warning(f"‚ö†Ô∏è Received 403 Forbidden for {url} - site may be blocking automated requests")
                    # Try with a different User-Agent as fallback
                    fallback_headers = headers.copy()
                    fallback_headers["User-Agent"] = "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/122.0.0.0 Safari/537.36"
                    try:
                        response = await client.get(url, headers=fallback_headers)
                        if response.status_code == 403:
                            # Still 403 after fallback - site is blocking us
                            response.raise_for_status()
                    except httpx.HTTPStatusError:
                        # Re-raise the 403 error
                        raise
                
                response.raise_for_status()
                
                # Extract title and content
                content = response.text
                title = self._extract_title(content)
                
                return {
                    "title": title,
                    "content": content[:10000],  # Limit content length
                    "url": url,
                    "status_code": response.status_code
                }
                
        except httpx.HTTPStatusError as e:
            if e.response.status_code == 403:
                logger.error(f"‚ùå Failed to fetch content from {url}: 403 Forbidden - site is blocking automated requests")
            else:
                logger.error(f"‚ùå Failed to fetch content from {url}: HTTP {e.response.status_code}")
            return None
        except Exception as e:
            logger.error(f"‚ùå Failed to fetch content from {url}: {e}")
            return None
    
    def _extract_title(self, html_content: str) -> str:
        """Extract title from HTML content"""
        try:
            import re
            title_match = re.search(r'<title[^>]*>([^<]+)</title>', html_content, re.IGNORECASE)
            if title_match:
                return title_match.group(1).strip()
            return "Unknown Title"
        except:
            return "Unknown Title"
    
    async def _rate_limit(self):
        """Apply rate limiting"""
        elapsed = asyncio.get_event_loop().time() - self.last_request_time
        if elapsed < self.rate_limit:
            wait_time = self.rate_limit - elapsed
            await asyncio.sleep(wait_time)
        self.last_request_time = asyncio.get_event_loop().time()
    
    async def _search_searxng(self, query: str, limit: int) -> List[Dict[str, Any]]:
        """Search using SearXNG JSON API"""
        try:
            # SearXNG search endpoint
            search_url = f"{self.searxng_url}/search"
            params = {
                "q": query,
                "format": "json",
                "categories": "general",
                "engines": "google,duckduckgo",  # Use Google and DuckDuckGo only
                "lang": "en",
                "pageno": 1
            }
            
            logger.info(f"üåê SearXNG request URL: {search_url}")
            logger.info(f"üåê SearXNG request params: {params}")
            
            headers = {
                "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/122.0.0.0 Safari/537.36",
                "Accept": "application/json,text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8",
                "Accept-Language": "en-US,en;q=0.9",
                "Accept-Encoding": "gzip, deflate, br",
                "Connection": "keep-alive",
                "X-Forwarded-For": "172.18.0.1",  # Docker bridge gateway IP for bot detection
                "X-Real-IP": "172.18.0.1"  # Required by SearXNG bot detection
            }
            
            async with httpx.AsyncClient(timeout=15.0, headers=headers) as client:
                logger.info(f"üåê Making HTTP request to SearXNG...")
                response = await client.get(search_url, params=params)
                logger.info(f"üåê SearXNG HTTP response status: {response.status_code}")
                response.raise_for_status()
                data = response.json()
                logger.info(f"üåê SearXNG response data keys: {list(data.keys())}")
            
            results = []
            search_results = data.get("results", [])
            
            for i, result in enumerate(search_results[:limit]):
                # Extract data from SearXNG result
                title = result.get("title", "").strip()
                url = result.get("url", "").strip()
                content = result.get("content", "").strip()
                engine = result.get("engine", "unknown")
                
                # Skip invalid results
                if not title or not url or len(title) < 3:
                    continue
                
                # Create result dict with enhanced citation fields
                results.append({
                    "title": title[:200],  # Limit title length
                    "url": url,
                    "snippet": content[:500] if content else f"Search result from {engine}",
                    "source": self._extract_domain(url),
                    "relevance_score": max(0.9 - (i * 0.05), 0.1),  # Decreasing relevance
                    "published_date": result.get("publishedDate", None),
                    "engine": engine,
                    # Enhanced citation support
                    "citation_type": "webpage",
                    "domain": self._extract_domain(url),
                    "accessed_date": datetime.now().strftime("%Y-%m-%d"),
                    "search_rank": i + 1,
                    "confidence": max(0.9 - (i * 0.05), 0.1)
                })
            
            logger.info(f"‚úÖ SearXNG returned {len(results)} results")
            return results
            
        except Exception as e:
            logger.error(f"‚ùå SearXNG search failed: {e}")
            # Return a fallback result
            return [{
                "title": f"Web search for: {query}",
                "url": f"{self.searxng_url}/search?q={quote_plus(query)}",
                "snippet": f"SearXNG search for '{query}'. This is a fallback result - check SearXNG service status.",
                "source": "searxng",
                "relevance_score": 0.5,
                "engine": "searxng"
            }]
    
    def _extract_domain(self, url: str) -> str:
        """Extract domain from URL"""
        try:
            from urllib.parse import urlparse
            parsed = urlparse(url)
            return parsed.netloc
        except:
            return "unknown"
    
    async def _analyze_relevance(self, query: str, results: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """Analyze search results for relevance to the query"""
        try:
            # For now, return results as-is since relevance analysis may need implementation
            # In the future, this could use LLM to score relevance
            return results
            
        except Exception as e:
            logger.error(f"‚ùå Relevance analysis failed: {e}")
            return results


# Global instance for use by tool registry
_web_content_instance = None


async def _get_web_content():
    """Get global web content instance"""
    global _web_content_instance
    if _web_content_instance is None:
        _web_content_instance = WebContentTools()
    return _web_content_instance


async def search_web(query: str, limit: int = 10, analyze_relevance: bool = True, user_id: str = None) -> Dict[str, Any]:
    """LangGraph tool function: Search the web using SearXNG"""
    tools_instance = await _get_web_content()
    return await tools_instance.search_web(query, limit, analyze_relevance, user_id)


async def analyze_and_ingest_url(urls: List[str], criteria: str = "relevant and authoritative", max_urls: int = 5, user_id: str = None) -> Dict[str, Any]:
    """LangGraph tool function: Analyze and ingest URLs"""
    tools_instance = await _get_web_content()
    return await tools_instance.analyze_and_ingest(urls, criteria, max_urls, user_id)


async def crawl_web_content(url: str = None, urls: List[str] = None, user_id: str = None) -> Dict[str, Any]:
    """
    LangGraph tool function: Crawl web content from URL(s)
    
    **ROOSEVELT FIX**: Accepts either single URL or multiple URLs to match tool schema
    
    Args:
        url: Single URL to crawl (optional)
        urls: List of URLs to crawl (optional)
        user_id: User ID for permission checks
        
    Returns:
        Dict with crawled content or error message
    """
    tools_instance = await _get_web_content()
    
    # Handle both single URL and multiple URLs
    if url and not urls:
        # Single URL provided
        return await tools_instance._fetch_web_content(url)
    elif urls and not url:
        # Multiple URLs provided - crawl each and combine results
        results = []
        for single_url in urls:
            try:
                result = await tools_instance._fetch_web_content(single_url)
                results.append(result)
            except Exception as e:
                logger.error(f"‚ùå Failed to crawl {single_url}: {e}")
                results.append({"error": str(e), "url": single_url})
        
        # Combine all results
        combined_content = "\n\n---\n\n".join([
            r.get("content", r.get("error", "No content")) for r in results
        ])
        return {
            "success": True,
            "content": combined_content,
            "urls_crawled": len(results),
            "results": results
        }
    elif url and urls:
        # Both provided - use urls list and append url to it
        all_urls = urls + [url]
        return await crawl_web_content(urls=all_urls, user_id=user_id)
    else:
        # Neither provided
        return {"error": "Must provide either 'url' or 'urls' parameter"}


async def search_and_crawl(query: str, max_results: int = 15, user_id: str = None) -> Dict[str, Any]:
    """LangGraph tool function: Search web and crawl top results"""
    tools_instance = await _get_web_content()
    # This combines search_web and analyze_and_ingest
    search_result = await tools_instance.search_web(query, max_results, True, user_id)
    if search_result.get("success"):
        urls = [result["url"] for result in search_result.get("results", [])[:max_results]]
        logger.info(f"üì• Attempting to crawl {len(urls)} URLs from {len(search_result.get('results', []))} search results")
        crawled_content = []
        if urls:
            # Try to crawl URLs directly - more efficient than going through analyze_and_ingest
            for idx, url in enumerate(urls[:max_results], 1):
                logger.info(f"üï∑Ô∏è Crawling URL {idx}/{len(urls)}: {url[:80]}...")
                try:
                    content_result = await tools_instance._fetch_web_content(url)
                    if content_result:
                        crawled_content.append({
                            "url": url,
                            "title": content_result.get("title", "Unknown"),
                            "content": content_result.get("content", ""),
                            "html": content_result.get("content", "")  # Keep for internal use, not sent to proto
                        })
                        logger.info(f"‚úÖ Successfully crawled {idx}/{len(urls)}: {url[:80]}...")
                    else:
                        logger.warning(f"‚ö†Ô∏è No content returned for {idx}/{len(urls)}: {url[:80]}...")
                except Exception as e:
                    logger.warning(f"‚ö†Ô∏è Failed to crawl {idx}/{len(urls)} {url[:80]}...: {e}")
                    # Continue with other URLs even if one fails
                    continue
        
        logger.info(f"üìä Crawl summary: Attempted {len(urls)} URLs, successfully crawled {len(crawled_content)} URLs")
        
        # Return search results even if crawling failed - search results are still valuable
        return {
            "success": True,
            "search_results": search_result.get("results", []),
            "crawled_content": crawled_content,  # Match gRPC service expectation
            "ingested_content": crawled_content,  # Keep for backward compatibility
            "summary": f"Found {len(search_result.get('results', []))} search results, crawled {len(crawled_content)} URLs"
        }
    return {"success": False, "error": "Search failed", "search_results": [], "crawled_content": []}